{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pathlib\nimport gc\nimport math\n\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T13:20:33.770723Z","iopub.execute_input":"2023-09-24T13:20:33.771273Z","iopub.status.idle":"2023-09-24T13:20:36.101272Z","shell.execute_reply.started":"2023-09-24T13:20:33.771223Z","shell.execute_reply":"2023-09-24T13:20:36.100238Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"INPUT_PATH = pathlib.Path('/kaggle/input/stanford-ribonanza-rna-folding-converted')\nWORKING_PATH = pathlib.Path('/kaggle/working/')\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2023-09-24T13:20:37.066819Z","iopub.execute_input":"2023-09-24T13:20:37.067368Z","iopub.status.idle":"2023-09-24T13:20:37.101254Z","shell.execute_reply.started":"2023-09-24T13:20:37.067338Z","shell.execute_reply":"2023-09-24T13:20:37.100098Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"full_df = pd.read_parquet(INPUT_PATH/\"train_data.parquet\")\ntest_df = pd.read_parquet(INPUT_PATH/\"test_sequences.parquet\")\n\ndf_2A3 = full_df[full_df.experiment_type =='2A3_MaP'].reset_index(drop=True)\ndf_DMS = full_df[full_df.experiment_type =='DMS_MaP'].reset_index(drop=True)\ntrain_2A3, val_2A3, train_DMS, val_DMS= train_test_split(df_2A3, df_DMS, test_size=0.1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T13:20:37.440933Z","iopub.execute_input":"2023-09-24T13:20:37.441941Z","iopub.status.idle":"2023-09-24T13:21:02.366698Z","shell.execute_reply.started":"2023-09-24T13:20:37.441902Z","shell.execute_reply":"2023-09-24T13:21:02.365643Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class RNA_Dataset(Dataset):\n    def __init__(self, df_2A3, df_DMS):\n        # filter noisy data for now\n        df_2A3 = df_2A3[(df_2A3.SN_filter > 0) & (df_DMS.SN_filter > 0)]\n        \n        self.seq_map = {'A':1, 'C':2, 'G':3, 'U':4}\n        self.seqs = df_2A3.sequence.values\n        self.react_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n                                 'reactivity_0' in c]].values\n        self.react_DMS = df_DMS[[c for c in df_DMS.columns if \\\n                                 'reactivity_0' in c]].values\n        \n    def __len__(self):\n        return len(self.seqs)\n        \n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        seq_idx = torch.tensor([self.seq_map[s] for s in seq], dtype=torch.long)\n        labels = torch.tensor(np.stack([self.react_2A3[idx],\n                                           self.react_DMS[idx]], -1), dtype=torch.float32)\n        return seq_idx, labels\n        \ndef collate_fn(data):\n    seq_idx, labels = zip(*data)\n    padded_seqs = nn.utils.rnn.pad_sequence(seq_idx, batch_first=True)\n    B, T = padded_seqs.shape\n    labels = torch.stack(labels)[:, :T]\n    return padded_seqs, labels","metadata":{"execution":{"iopub.status.busy":"2023-09-24T13:21:02.368759Z","iopub.execute_input":"2023-09-24T13:21:02.369142Z","iopub.status.idle":"2023-09-24T13:21:02.380541Z","shell.execute_reply.started":"2023-09-24T13:21:02.369107Z","shell.execute_reply":"2023-09-24T13:21:02.379505Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"vocab_size = 5 # the 4 bases + padding\nemb_dim = 256\nnum_layers = 14\nnhead=8\nbatch_size = 64\n\n# we have to use fixed Positions because training data is \n# shorter than test data\nclass PositionEncoding(nn.Module):\n    def __init__(self, emb_dim, max_len=512):\n        super().__init__()\n        positions = torch.arange(max_len).unsqueeze(1)\n        evens = torch.arange(0, emb_dim, 2)\n        frequencies = torch.exp(evens * (-math.log(10_000)/emb_dim))\n        pos_embs = torch.zeros(max_len, emb_dim)\n        pos_embs[:, 0::2] = torch.sin(positions * frequencies)\n        pos_embs[:, 1::2] = torch.cos(positions * frequencies)\n        self.register_buffer('pos_emb', pos_embs)\n        \n    def forward(self, x):\n        return x + self.pos_emb[:x.size(1)]\n                \nclass RNA_Transformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_emb = nn.Embedding(vocab_size, emb_dim)\n        self.pos_emb = PositionEncoding(emb_dim)\n        enc_layer = nn.TransformerEncoderLayer(emb_dim, nhead,\n                                               dim_feedforward=4*emb_dim,\n                                               batch_first=True, norm_first=True,\n                                               activation=\"gelu\")\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.regression_head = nn.Linear(emb_dim, 2)\n        \n    def forward(self, x, targets=None):\n        B, T = x.shape\n        x = self.token_emb(x)\n        x = self.pos_emb(x)\n        x = self.encoder(x)\n        preds = self.regression_head(x)\n        \n        if targets is None:\n            loss = None\n        else:\n            preds = preds.view(B*T, 2)\n            targets = targets.contiguous().view(B*T, 2).clamp(0, 1)\n            loss = F.l1_loss(preds, targets, reduction='none')\n            loss = loss[~loss.isnan()].mean()\n            \n        return preds, loss","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:00:40.674520Z","iopub.execute_input":"2023-09-24T17:00:40.674916Z","iopub.status.idle":"2023-09-24T17:00:40.688510Z","shell.execute_reply.started":"2023-09-24T17:00:40.674884Z","shell.execute_reply":"2023-09-24T17:00:40.687532Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_dataset, val_dataset = RNA_Dataset(train_2A3, train_DMS), RNA_Dataset(val_2A3, val_DMS)\ntrainloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\nvalidloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:00:43.493257Z","iopub.execute_input":"2023-09-24T17:00:43.493688Z","iopub.status.idle":"2023-09-24T17:00:45.051062Z","shell.execute_reply.started":"2023-09-24T17:00:43.493649Z","shell.execute_reply":"2023-09-24T17:00:45.049718Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model = RNA_Transformer() #torch.load(WORKING_PATH/\"best_model.pth\")\nmodel.to(device);","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:00:52.266548Z","iopub.execute_input":"2023-09-24T17:00:52.266919Z","iopub.status.idle":"2023-09-24T17:00:52.332738Z","shell.execute_reply.started":"2023-09-24T17:00:52.266890Z","shell.execute_reply":"2023-09-24T17:00:52.331721Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"epochs = 30\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\ntrain_steps = epochs * len(trainloader)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-4, total_steps=train_steps, pct_start=0.02)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:06:55.994509Z","iopub.execute_input":"2023-09-24T17:06:55.994887Z","iopub.status.idle":"2023-09-24T17:06:56.001802Z","shell.execute_reply.started":"2023-09-24T17:06:55.994857Z","shell.execute_reply":"2023-09-24T17:06:56.000743Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef eval_loop():\n    model.eval()\n    losses = torch.zeros(len(validloader))\n    for i, (x, y) in tqdm(enumerate(validloader), total=len(validloader)):\n        _, loss = model(x.to(device), y.to(device))\n        losses[i] = loss.item()\n    model.train()\n    val_loss = losses.mean().item()\n    print(f\"Val Loss: {val_loss}\")\n    return val_loss\n            \neval_distance = 500\nmin_loss = 0.19\nn_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Training model with {n_params:,} parameters...\")\nloss_dict = {\"train_loss\": [], \"val_loss\": []}\nfor epoch in range(epochs):\n    losses = torch.zeros(len(trainloader))\n    pbar = tqdm(enumerate(trainloader), total=len(trainloader))\n    pbar.set_description(f\"Epoch {epoch}\")\n    for i, (x, y) in pbar:\n        _, loss = model(x.to(device), y.to(device))\n        losses[i] = loss.item()\n        \n        if i >= eval_distance and i % eval_distance == 0:\n            train_loss = losses[i-eval_distance:i].mean().item()\n            pbar.set_postfix({\"Loss\":  train_loss})\n        \n        optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n        optimizer.step()\n        scheduler.step()\n    val_loss = eval_loop()\n    loss_dict[\"train_loss\"].append(train_loss)\n    loss_dict[\"val_loss\"].append(val_loss)\n    if min_loss > val_loss:\n        min_loss = val_loss\n        torch.save(model, \"best_model.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:07:13.777099Z","iopub.execute_input":"2023-09-24T17:07:13.777468Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training model with 11,058,434 parameters...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 0:   0%|          | 0/2550 [00:00<?, ?it/s]/tmp/ipykernel_167/1184231916.py:32: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  nn.utils.clip_grad_norm(model.parameters(), 3.0)\nEpoch 0: 100%|██████████| 2550/2550 [11:50<00:00,  3.59it/s, Loss=0.242]\n100%|██████████| 284/284 [00:20<00:00, 14.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.2453475147485733\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 2550/2550 [11:49<00:00,  3.59it/s, Loss=0.227]\n100%|██████████| 284/284 [00:19<00:00, 14.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.22963878512382507\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 2550/2550 [11:49<00:00,  3.60it/s, Loss=0.22] \n100%|██████████| 284/284 [00:19<00:00, 14.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.22220367193222046\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 2550/2550 [11:49<00:00,  3.59it/s, Loss=0.216]\n100%|██████████| 284/284 [00:20<00:00, 14.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.22220933437347412\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 2550/2550 [11:49<00:00,  3.60it/s, Loss=0.212]\n100%|██████████| 284/284 [00:19<00:00, 14.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.21231424808502197\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 2550/2550 [11:46<00:00,  3.61it/s, Loss=0.208]\n100%|██████████| 284/284 [00:19<00:00, 14.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.2078811079263687\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 2550/2550 [11:46<00:00,  3.61it/s, Loss=0.205]\n100%|██████████| 284/284 [00:19<00:00, 14.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.20532453060150146\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 2550/2550 [11:47<00:00,  3.61it/s, Loss=0.201]\n100%|██████████| 284/284 [00:19<00:00, 14.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.2011735737323761\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8:  84%|████████▍ | 2141/2550 [09:55<01:55,  3.53it/s, Loss=0.199]","output_type":"stream"}]},{"cell_type":"code","source":"pd.DataFrame.from_dict(loss_dict)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:00:07.512656Z","iopub.status.idle":"2023-09-24T14:00:07.515144Z","shell.execute_reply.started":"2023-09-24T14:00:07.514865Z","shell.execute_reply":"2023-09-24T14:00:07.514889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}