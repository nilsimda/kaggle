{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pathlib\nimport gc\nimport math\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import Dataset, DataLoader, BatchSampler, RandomSampler\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-02T13:45:05.947362Z","iopub.execute_input":"2023-10-02T13:45:05.947691Z","iopub.status.idle":"2023-10-02T13:45:11.312001Z","shell.execute_reply.started":"2023-10-02T13:45:05.947662Z","shell.execute_reply":"2023-10-02T13:45:11.311150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = pathlib.Path('/kaggle/input/stanford-ribonanza-rna-folding-converted')\nMODEL_PATH = pathlib.Path('/kaggle/input/rna-folding-model/')\nWORKING_PATH = pathlib.Path('/kaggle/working/')\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2023-10-02T13:45:11.313941Z","iopub.execute_input":"2023-10-02T13:45:11.314548Z","iopub.status.idle":"2023-10-02T13:45:11.320862Z","shell.execute_reply.started":"2023-10-02T13:45:11.314510Z","shell.execute_reply":"2023-10-02T13:45:11.319669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_df = pd.read_parquet(INPUT_PATH/\"train_data.parquet\")\ntest_df = pd.read_parquet(INPUT_PATH/\"test_sequences.parquet\")\n\ndf_2A3 = full_df[full_df.experiment_type =='2A3_MaP'].reset_index(drop=True)\ndf_DMS = full_df[full_df.experiment_type =='DMS_MaP'].reset_index(drop=True)\ntrain_2A3, val_2A3, train_DMS, val_DMS= train_test_split(df_2A3, df_DMS, test_size=0.1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T13:45:13.684026Z","iopub.execute_input":"2023-10-02T13:45:13.685294Z","iopub.status.idle":"2023-10-02T13:45:39.528763Z","shell.execute_reply.started":"2023-10-02T13:45:13.685211Z","shell.execute_reply":"2023-10-02T13:45:39.527525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNA_Dataset(Dataset):\n    def __init__(self, df_2A3, df_DMS):\n        # filter noisy data for now\n        df_2A3 = df_2A3[(df_2A3.SN_filter > 0) & (df_DMS.SN_filter > 0)]\n        \n        self.seq_map = {'A':1, 'C':2, 'G':3, 'U':4}\n        self.seqs = df_2A3.sequence.values\n        self.react_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n                                 'reactivity_0' in c]].values\n        self.react_DMS = df_DMS[[c for c in df_DMS.columns if \\\n                                 'reactivity_0' in c]].values\n        \n    def __len__(self):\n        return len(self.seqs)\n        \n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        seq_idx = torch.tensor([self.seq_map[s] for s in seq], dtype=torch.long)\n        labels = torch.tensor(np.stack([self.react_2A3[idx],\n                                           self.react_DMS[idx]], -1), dtype=torch.float32)\n        return seq_idx, labels\n    \n# Useful for sampling batches of similar lengths to minimize padding\nclass GroupLengthBatchSampler(BatchSampler):\n    def __iter__(self):\n        dataset = self.sampler.data_source\n        indices = [idx for idx in self.sampler]\n\n        step = 100 * self.batch_size\n        for i in range(0, len(dataset), step):\n            pool = indices[i:i+step]\n            pool = sorted(pool, key=lambda x: len(dataset[x][0]))\n            for j in range(0, len(pool), self.batch_size):\n                if j + self.batch_size > len(pool): # assume drop_last=True\n                    break\n                yield pool[j:j+self.batch_size]\n        \ndef collate_fn(data):\n    seq_idx, labels = zip(*data)\n    padded_seqs = nn.utils.rnn.pad_sequence(seq_idx, batch_first=True)\n    B, T = padded_seqs.shape\n    labels = torch.stack(labels)[:, :T]\n    return padded_seqs, labels","metadata":{"execution":{"iopub.status.busy":"2023-10-02T13:45:43.623678Z","iopub.execute_input":"2023-10-02T13:45:43.624021Z","iopub.status.idle":"2023-10-02T13:45:43.638951Z","shell.execute_reply.started":"2023-10-02T13:45:43.623996Z","shell.execute_reply":"2023-10-02T13:45:43.637395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = 5 # the 4 bases + padding\nemb_dim = 256\nnum_layers = 12\nnhead=8\nbatch_size = 128\n\n# we have to use fixed Positions because training data is \n# shorter than test data\nclass PositionEncoding(nn.Module):\n    def __init__(self, emb_dim, max_len=512):\n        super().__init__()\n        positions = torch.arange(max_len).unsqueeze(1)\n        evens = torch.arange(0, emb_dim, 2)\n        frequencies = torch.exp(evens * (-math.log(10_000)/emb_dim))\n        pos_embs = torch.zeros(max_len, emb_dim)\n        pos_embs[:, 0::2] = torch.sin(positions * frequencies)\n        pos_embs[:, 1::2] = torch.cos(positions * frequencies)\n        self.register_buffer('pos_emb', pos_embs)\n        \n    def forward(self, x):\n        return x + self.pos_emb[:x.size(1)]\n                \nclass RNA_Transformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_emb = nn.Embedding(vocab_size, emb_dim)\n        self.pos_emb = PositionEncoding(emb_dim)\n        enc_layer = nn.TransformerEncoderLayer(emb_dim, nhead,\n                                               dim_feedforward=4*emb_dim,\n                                               batch_first=True, norm_first=True,\n                                               activation=\"gelu\")\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.regression_head = nn.Linear(emb_dim, 2)\n        \n    def forward(self, x, targets=None):\n        B, T = x.shape\n        x = self.token_emb(x)\n        x = self.pos_emb(x)\n        x = self.encoder(x)\n        preds = self.regression_head(x)\n        \n        if targets is None:\n            loss = None\n        else:\n            preds = preds.view(B*T, 2)\n            targets = targets.contiguous().view(B*T, 2).clamp(0, 1)\n            loss = F.l1_loss(preds, targets, reduction='none')\n            loss = loss[~loss.isnan()].mean()\n            \n        return preds, loss","metadata":{"execution":{"iopub.status.busy":"2023-10-02T13:45:48.489837Z","iopub.execute_input":"2023-10-02T13:45:48.490219Z","iopub.status.idle":"2023-10-02T13:45:48.503120Z","shell.execute_reply.started":"2023-10-02T13:45:48.490191Z","shell.execute_reply":"2023-10-02T13:45:48.502290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset, val_dataset = RNA_Dataset(train_2A3, train_DMS), RNA_Dataset(val_2A3, val_DMS)\ntrainsampler = GroupLengthBatchSampler(RandomSampler(train_dataset), batch_size, drop_last=True)\nvalsampler = GroupLengthBatchSampler(RandomSampler(val_dataset), batch_size, drop_last=True)\ntrainloader = DataLoader(train_dataset, batch_sampler=trainsampler, collate_fn=collate_fn)\nvalidloader = DataLoader(val_dataset, batch_sampler=valsampler, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T13:45:58.316562Z","iopub.execute_input":"2023-10-02T13:45:58.316972Z","iopub.status.idle":"2023-10-02T13:45:59.134437Z","shell.execute_reply.started":"2023-10-02T13:45:58.316942Z","shell.execute_reply":"2023-10-02T13:45:59.133102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.load(MODEL_PATH/\"best_model.pth\", map_location=device)\nmodel.to(device);","metadata":{"execution":{"iopub.status.busy":"2023-10-02T13:46:08.203337Z","iopub.execute_input":"2023-10-02T13:46:08.203723Z","iopub.status.idle":"2023-10-02T13:46:08.690097Z","shell.execute_reply.started":"2023-10-02T13:46:08.203695Z","shell.execute_reply":"2023-10-02T13:46:08.688487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 30\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\ntrain_steps = epochs * len(trainloader)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_steps)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:01:08.492165Z","iopub.execute_input":"2023-09-28T12:01:08.492580Z","iopub.status.idle":"2023-09-28T12:01:08.500369Z","shell.execute_reply.started":"2023-09-28T12:01:08.492547Z","shell.execute_reply":"2023-09-28T12:01:08.499220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef eval_loop():\n    model.eval()\n    losses = torch.zeros(len(validloader))\n    for i, (x, y) in tqdm(enumerate(validloader), total=len(validloader)):\n        _, loss = model(x.to(device), y.to(device))\n        losses[i] = loss.item()\n    model.train()\n    val_loss = losses.mean().item()\n    print(f\"Val Loss: {val_loss}\")\n    return val_loss\n            \neval_distance = 500\nmin_loss = 0.185\nn_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Training model with {n_params:,} parameters...\")\nloss_dict = {\"train_loss\": [], \"val_loss\": []}\nfor epoch in range(epochs):\n    losses = torch.zeros(len(trainloader))\n    pbar = tqdm(enumerate(trainloader), total=len(trainloader))\n    pbar.set_description(f\"Epoch {epoch}\")\n    for i, (x, y) in pbar:\n        _, loss = model(x.to(device), y.to(device))\n        losses[i] = loss.item()\n        \n        if i >= eval_distance and i % eval_distance == 0:\n            train_loss = losses[i-eval_distance:i].mean().item()\n            pbar.set_postfix({\"Loss\":  train_loss})\n        \n        optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n        optimizer.step()\n        scheduler.step()\n    val_loss = eval_loop()\n    loss_dict[\"train_loss\"].append(train_loss)\n    loss_dict[\"val_loss\"].append(val_loss)\n    if min_loss > val_loss:\n        min_loss = val_loss\n        torch.save(model, WORKING_PATH/\"best_model.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:01:11.401844Z","iopub.execute_input":"2023-09-28T12:01:11.402517Z","iopub.status.idle":"2023-09-28T12:01:42.444085Z","shell.execute_reply.started":"2023-09-28T12:01:11.402482Z","shell.execute_reply":"2023-09-28T12:01:42.442437Z"},"trusted":true},"execution_count":null,"outputs":[]}]}